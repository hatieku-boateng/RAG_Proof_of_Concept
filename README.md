# AI Knowledge Assistant (Streamlit RAG over OpenAI Vector Stores)

This project is a local Retrieval-Augmented Generation (RAG) chat application built with Streamlit and the OpenAI Python SDK.

It lets you:

- Upload documents into an OpenAI **Vector Store** (your “knowledge base”)
- Select a knowledge base in a Streamlit UI
- Ask questions in a chat interface
- Receive answers generated by an OpenAI Assistant using the `file_search` tool to retrieve relevant content

The goal of this README is to explain the project end-to-end so a new developer can understand:

- The RAG concept
- The architecture and data flow
- What each file does
- How to configure, run, and troubleshoot the app

---

## 1) Conceptual overview

### What is RAG?

RAG (Retrieval-Augmented Generation) combines:

- **Retrieval**: search a document collection for relevant passages
- **Generation**: use an LLM to craft a final answer grounded in the retrieved passages

This reduces hallucinations and makes answers traceable to your documents.

### How RAG is implemented here

This project uses OpenAI’s hosted retrieval via:

- **Vector Stores**: storage + indexing for your uploaded documents
- **Assistants API**: creates an assistant that can call tools
- **`file_search` tool**: retrieves relevant content from a specified vector store

At runtime, the app links the selected vector store to the assistant, so the assistant can retrieve from the correct knowledge base.

---

## 2) Repository structure

```text
OpenAI/
  streamlit_app.py        # Streamlit RAG chat UI
  notebooks/
    openai_1.ipynb        # Notebook for creating/managing vector stores and uploading documents
  scripts/                # Helper scripts for managing the PU Repo vector store
  pu_repo/               # Pentecost University document repository (knowledge base sources)
  requirements.txt       # Python dependencies
  web/                   # (Optional) Next.js (React) frontend – not covered in this README
  .gitignore             # Excludes .env, .venv, and API keys directory
  .env                   # Local config (not committed)
```

---

## 3) Architecture and data flow

### High-level flow

1. **User selects a vector store** (knowledge base) in the sidebar
2. App creates an **Assistant** configured with:
   - A model (`OPENAI_MODEL`, with safe fallbacks)
   - `file_search` tool enabled
   - `tool_resources.file_search.vector_store_ids = [selected_store_id]`
3. App creates a **Thread** to hold the conversation
4. User sends a message
5. The app:
   - Adds the user message to the thread
   - Starts an assistant run
   - Polls until the run completes
   - Fetches the assistant’s reply and renders it in Streamlit

### What state is stored

In Streamlit `st.session_state`:

- `messages`: local chat transcript for display
- `assistant_id`: the current assistant for the selected vector store
- `thread_id`: the current conversation thread
- `selected_vector_store`: tracks which store is currently active
- `pending_prompt`: used to convert “suggested question” button clicks into real chat queries

---

## 4) Configuration (environment variables)

Create a `.env` file in the project root (never commit it):

```env
OPENAI_API_KEY=your_api_key_here

# Model used by Assistants
OPENAI_MODEL=gpt-4o-mini

# Optional (used by the notebook)
OPENAI_MODEL_EMBEDDING=text-embedding-3-small

# Optional: if set, the notebook will use this existing vector store by ID
VECTOR_STORE_ID=vs_...
```

### Model selection and cost

- The app reads `OPENAI_MODEL`.
- If the model is not supported by the Assistants workflow, it falls back to a safe supported model.
- If your goal is to reduce cost, set a cheaper model explicitly in `.env`.

Note: model availability and pricing depend on your OpenAI account and region.

---

## 5) Setup and installation

### Create and activate a virtual environment

Windows PowerShell:

```bash
python -m venv .venv
.\.venv\Scripts\activate
```

### Install dependencies

```bash
python -m pip install -r requirements.txt
```

---

## 6) Preparing your knowledge base (Vector Store)

The Streamlit app only queries vector stores that already exist in your OpenAI account.

Use `notebooks/openai_1.ipynb` to:

- List and optionally delete old vector stores created during experimentation
- Create or retrieve the main **`PU Repo`** vector store
- Upload local documents from the `pu_repo/` folder and attach them to `PU Repo`
- Clear and re-add files (useful when refreshing content)

### Common workflow (PU Repo)

1. Open `notebooks/openai_1.ipynb`.
2. Run cells from the top until the helpers are defined.
3. (Optional) Run the **danger** cell that lists vector stores and, if you set `CONFIRM_DELETE = True`, deletes the old ones (for example `Course outlines` or `my-knowledge-base`).
4. Run the "Creating a vector store" cell to create or select the `PU Repo` vector store.
5. Upload documents by either:

   ```python
   FILE_PATHS = [
       r"pu_repo\\some_file.docx",
   ]
   ```

   or by using the "clear + re-add" cell to sync **all** files under `pu_repo/`:

   - The cell deletes all current files attached to `PU Repo` and then uploads everything under `pu_repo/`.

---

## 7) Running the application

### Option A: Streamlit app

Run Streamlit:

```bash
streamlit run streamlit_app.py
```

Open the local URL printed by Streamlit.

### Important note

Running `python streamlit_app.py` will not start Streamlit. This repo includes a guard that prints the correct command.

---

## 8) UI behavior

### Suggested questions

When the chat is empty, the UI shows suggested questions.

Clicking a suggestion:

- sets `st.session_state.pending_prompt`
- reruns the script
- routes the suggestion through the same handler as typed input

This ensures suggestions behave like real queries and still allow follow-up questions in the chat bar.

### Selecting a different knowledge base

When you change the vector store selection:

- the app creates a new assistant bound to the new store
- it resets the thread (fresh conversation)

---

## 9) Troubleshooting

### “No vector stores found”

- Create/upload documents using `notebooks/openai_1.ipynb`
- Confirm the API key is valid and has quota

### “Error creating assistant”

- Confirm your OpenAI account supports Assistants
- Try a different `OPENAI_MODEL` value

### App starts but answers are irrelevant

- Confirm you uploaded the correct documents to the selected vector store
- Refresh the vector store using the notebook “clear + re-add” cell

### Deprecation warning about Assistants API

You may see warnings indicating the Assistants API is being deprecated in favor of the Responses API.
This app currently uses Assistants; a future enhancement would be migrating the run logic to Responses.

---

## 10) Security and operational guidance

- Never commit `.env` or API keys
- Rotate keys immediately if exposed
- Treat uploaded documents as sensitive data (they are sent to OpenAI for processing)

---

## 11) Extension ideas

- Add citations rendering (show which retrieved chunks were used)
- Add upload management directly in Streamlit (upload files + attach to a vector store)
- Add user authentication if deploying beyond local use
- Migrate from Assistants API to the Responses API when needed

---

Built with Streamlit and the OpenAI Python SDK.
